{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "resnet18.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXpElTpuvsp6"
      },
      "source": [
        "import torch \n",
        "import torchvision\n",
        "import tqdm\n",
        "from torchvision import transforms as transforms\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1ck8EDTwd2R",
        "outputId": "83a40c2a-5f88-4c11-e415-d53cc4ee3e0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLIRIdtswgJA",
        "outputId": "11fb817b-bca8-4d03-c08d-3c0fe479bbb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "dataset = torchvision.datasets.CIFAR10\n",
        "data_dir = '/content/drive/My Drive/task2/data'\n",
        "batch_size = 125\n",
        "num_workers = 20\n",
        "device = torch.device('cuda')\n",
        "cuda = device.type == 'cuda'\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "train_dataset = dataset(data_dir, train=True, download=True, transform=transform)\n",
        "train_iter = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=num_workers, pin_memory=cuda)\n",
        "\n",
        "val_dataset = dataset(data_dir, download=True, train=False, transform=transform)\n",
        "val_iter = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
        "                                       pin_memory=cuda)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWjz7V4q2JKO",
        "outputId": "72032763-b7c8-4593-e353-e0344c809990",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUjqv5Hk31cU",
        "outputId": "85928348-9b85-42de-deb4-5b27393d3b5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVgfV85a4aYK"
      },
      "source": [
        "model.fc = torch.nn.Linear(512, 10)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71ZeC-dg4cXz",
        "outputId": "79b3e835-3932-4ebb-ab32-bad80aeaa76c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_BtHoUZ5nc8",
        "outputId": "9af334c0-ff9d-4549-d197-4d5e12ca5905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScybFmXb9P8D"
      },
      "source": [
        "loss = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, momentum=9e-1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w7R86B86A6x"
      },
      "source": [
        "\n",
        "\n",
        "def train_network(network, loss, optimizer, train_iter, val_iter, num_epochs, device='cpu', start_epoch=0,\n",
        "                 checkpoints=False, out_dir=None):\n",
        "    \"\"\"Model training\"\"\"\n",
        "    training_cycles = dict(loss=[], acc=[], val_loss=[], val_acc=[])\n",
        "    best_epoch, best_acc = 0, 0.0\n",
        "    \n",
        "    \n",
        "    for epoch_ in range(start_epoch, start_epoch+num_epochs):\n",
        "        \n",
        "        print(\"Epoch %d \"% (epoch_+1))\n",
        "        \n",
        "        \n",
        "        ## Training \n",
        "        print(\"Training \", flush=True)\n",
        "        correct, avg_loss, total = 0, 0, 0 \n",
        "        network.train()\n",
        "        data_iter = tqdm.tqdm(train_iter, desc=\"Loss: %.3f, Acc:%.3f\"% (0, 0))\n",
        "        \n",
        "        \n",
        "        for batch_x , batch_y in data_iter:\n",
        "            optimizer.zero_grad() # initialized zero gradient\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "            \n",
        "            # forward propogation\n",
        "            res = network(batch_x) \n",
        "            # loss \n",
        "            l = loss(res, batch_y)\n",
        "            # back progation\n",
        "            l.backward()\n",
        "            optimizer.step()\n",
        "            res = torch.argmax(res, dim=-1)\n",
        "            correct += torch.sum(res == batch_y).data.cpu().numpy()\n",
        "            total += batch_x.shape[0]\n",
        "            avg_loss += l.data.cpu().numpy() * batch_x.shape[0]\n",
        "            data_iter.set_description('Loss: %.3f, Acc: %.3f' % (avg_loss / total, correct / total))\n",
        "\n",
        "        training_cycles['loss'].append(avg_loss/total)\n",
        "        training_cycles['acc'].append(correct / total)\n",
        "        \n",
        "        ## Validation \n",
        "        print(\"Validation \", flush=True)\n",
        "        network.eval()\n",
        "        correct, avg_loss, total = 0, 0, 0\n",
        "        data_iter = tqdm.tqdm(val_iter, desc=\"Loss: %.3f, Acc:%.3f\" %(0, 0))\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in data_iter:\n",
        "                batch_x = batch_x.to(device)\n",
        "                batch_y = batch_y.to(device)\n",
        "                res = network(batch_x)\n",
        "                l = loss(res, batch_y)\n",
        "                res = torch.argmax(res, dim=-1)\n",
        "                correct += torch.sum(res == batch_y).data.cpu().numpy()\n",
        "                total += batch_x.shape[0]\n",
        "                avg_loss += l.data.cpu().numpy() * batch_x.shape[0]\n",
        "                data_iter.set_description('Loss: %.3f, Acc: %.3f' % (avg_loss / total, correct / total))\n",
        "        training_cycles['val_loss'].append(avg_loss/total)\n",
        "        training_cycles['val_acc'].append(correct/total)  \n",
        "        if (correct/total) > best_acc:\n",
        "            best_epoch = epoch_\n",
        "            best_acc = correct/total\n",
        "        if checkpoints and out_dir is not None:\n",
        "            if not os.path.isdir(out_dir):\n",
        "                os.makedirs(out_dirs)\n",
        "            save_path = os.path.join(out_dir, 'model_.pth') # saving only last epoch \n",
        "            torch.save(network.state_dict(), save_path)\n",
        "    \n",
        "    print(\"Best epoch %d , validation _accuracy %.2f\" %(best_epoch+1, best_acc*100))\n",
        "    return training_cycles"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQMFw3Rn80DN",
        "outputId": "0706296d-92a5-432e-d559-e80168576511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        }
      },
      "source": [
        "load_path = None\n",
        "checkpoints= True\n",
        "out_dirs = '/content/drive/My Drive/task2/model'\n",
        "\n",
        "\n",
        "\n",
        "start_epoch = 0\n",
        "\n",
        "if load_path is not None:\n",
        "    state_dict = torch.load(load_path)\n",
        "    model.load_state_dict(state_dict)\n",
        "else:\n",
        "    train_network(network=model, loss=loss, optimizer=optimizer, train_iter=train_iter, val_iter=val_iter,\n",
        "                  num_epochs=10, device=device, start_epoch=start_epoch, checkpoints=checkpoints, \n",
        "                  out_dir=out_dirs)\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.067, Acc: 0.281: 100%|██████████| 400/400 [02:35<00:00,  2.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 1.730, Acc: 0.366: 100%|██████████| 80/80 [00:20<00:00,  4.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 1.473, Acc: 0.456: 100%|██████████| 400/400 [02:33<00:00,  2.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 1.340, Acc: 0.510: 100%|██████████| 80/80 [00:20<00:00,  3.97it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 1.183, Acc: 0.574: 100%|██████████| 400/400 [02:33<00:00,  2.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 1.148, Acc: 0.584: 100%|██████████| 80/80 [00:20<00:00,  3.98it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.943, Acc: 0.667: 100%|██████████| 400/400 [02:33<00:00,  2.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 1.087, Acc: 0.635: 100%|██████████| 80/80 [00:19<00:00,  4.01it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.746, Acc: 0.737: 100%|██████████| 400/400 [02:33<00:00,  2.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.794, Acc: 0.719: 100%|██████████| 80/80 [00:19<00:00,  4.03it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.590, Acc: 0.791: 100%|██████████| 400/400 [02:33<00:00,  2.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.709, Acc: 0.759: 100%|██████████| 80/80 [00:20<00:00,  4.00it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.469, Acc: 0.834: 100%|██████████| 400/400 [02:33<00:00,  2.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.721, Acc: 0.753: 100%|██████████| 80/80 [00:19<00:00,  4.01it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.368, Acc: 0.872: 100%|██████████| 400/400 [02:32<00:00,  2.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.691, Acc: 0.779: 100%|██████████| 80/80 [00:20<00:00,  3.97it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.277, Acc: 0.902: 100%|██████████| 400/400 [02:33<00:00,  2.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.587, Acc: 0.813: 100%|██████████| 80/80 [00:20<00:00,  4.00it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.194, Acc: 0.931: 100%|██████████| 400/400 [02:32<00:00,  2.62it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.721, Acc: 0.791: 100%|██████████| 80/80 [00:19<00:00,  4.00it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best epoch 9 , validation _accuracy 81.26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXl1HBh09OTp",
        "outputId": "35a9544b-63a6-446f-c98d-8777fbbe4bc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOC16B8_5hS6"
      },
      "source": [
        "**Full Model: Accuracy report **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCvq3NGnnnXf"
      },
      "source": [
        "for batch_x, batch_y in val_iter:\n",
        "    batch_x = batch_x.to(device)\n",
        "    labels = batch_y.cpu().data.numpy()\n",
        "    predicted_labels = model(batch_x)\n",
        "    predicted_labels = torch.argmax(predicted_labels, dim=-1)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqZiL8ASn_8r"
      },
      "source": [
        "predicted_labels = np.squeeze(predicted_labels.cpu().data.numpy())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh_AqAcfnMfG",
        "outputId": "d0485f6d-153f-4492-8e45-bdae14496e82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "source": [
        "\n",
        "print(confusion_matrix(labels, predicted_labels))\n",
        "print(classification_report(labels, predicted_labels))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 5  0  0  0  0  0  0  0  1  0]\n",
            " [ 0  8  0  0  1  0  0  0  0  0]\n",
            " [ 0  0 12  2  0  0  0  1  0  0]\n",
            " [ 0  0  1 16  0  0  0  0  0  0]\n",
            " [ 0  0  2  1  8  0  0  1  0  0]\n",
            " [ 0  0  1  8  0  6  0  2  0  0]\n",
            " [ 0  0  1  1  2  0 12  0  0  0]\n",
            " [ 0  0  1  0  0  1  0  8  0  0]\n",
            " [ 0  1  0  0  0  0  0  0  4  0]\n",
            " [ 0  0  1  0  0  0  0  0  0 17]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.83      0.91         6\n",
            "           1       0.89      0.89      0.89         9\n",
            "           2       0.63      0.80      0.71        15\n",
            "           3       0.57      0.94      0.71        17\n",
            "           4       0.73      0.67      0.70        12\n",
            "           5       0.86      0.35      0.50        17\n",
            "           6       1.00      0.75      0.86        16\n",
            "           7       0.67      0.80      0.73        10\n",
            "           8       0.80      0.80      0.80         5\n",
            "           9       1.00      0.94      0.97        18\n",
            "\n",
            "    accuracy                           0.77       125\n",
            "   macro avg       0.81      0.78      0.78       125\n",
            "weighted avg       0.81      0.77      0.76       125\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7bNJkW3n6xB"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpteEsmtzxEf"
      },
      "source": [
        "\n",
        "def recall_(labels, predicted_labels, class_):\n",
        "  \n",
        "  total = np.count_nonzero(labels == class_)\n",
        "  count = 0\n",
        "  for i in range(len(labels)):\n",
        "    if labels[i] == predicted_labels[i] == class_:\n",
        "      count+=1\n",
        "  return count/total"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u9YfsI_wtZ4"
      },
      "source": [
        "model_1_report = dict()\n",
        "\n",
        "for c in range(10):\n",
        "  model_1_report[c] = recall_(labels, predicted_labels, c)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfLFg_l72G60",
        "outputId": "d9177320-6de8-4646-ed90-52a5b6bc48e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "model_1_report"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.8333333333333334,\n",
              " 1: 0.8888888888888888,\n",
              " 2: 0.8,\n",
              " 3: 0.9411764705882353,\n",
              " 4: 0.6666666666666666,\n",
              " 5: 0.35294117647058826,\n",
              " 6: 0.75,\n",
              " 7: 0.8,\n",
              " 8: 0.8,\n",
              " 9: 0.9444444444444444}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbk6Nnip27Ht"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsgqG_hg3mSk"
      },
      "source": [
        "### Model 2 :: \n",
        "Using only the red channel as input (and  making a 3D tensor by copying the data of the red channel in the blue and green channels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWzwVC2p3ovJ",
        "outputId": "cb41d39d-007c-4032-fad5-34322e00a8cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "dataset = torchvision.datasets.CIFAR10\n",
        "data_dir = '/content/drive/My Drive/task2/dataR'\n",
        "\n",
        "def copy_red_channel(data_object):\n",
        "    data_object[1], data_object[2] = data_object[0], data_object[0]\n",
        "    return data_object\n",
        "\n",
        "\n",
        "transform_red = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(copy_red_channel)\n",
        "])\n",
        "\n",
        "train_R_dataset = dataset(data_dir, train=True, download=True, transform=transform_red)\n",
        "train_R_iter = torch.utils.data.DataLoader(train_R_dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=num_workers, pin_memory=cuda)\n",
        "\n",
        "val_R_dataset = dataset(data_dir, download=True, train=False, transform=transform)\n",
        "val_R_iter = torch.utils.data.DataLoader(val_R_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
        "                                       pin_memory=cuda)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGlhLaHv5GvX",
        "outputId": "66333cfa-739c-483b-a525-196f1d0a2b30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_R = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=False)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64-jaZXnRT-V"
      },
      "source": [
        "model_R.fc = torch.nn.Linear(512, 10)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtRxJUek5JGQ",
        "outputId": "2ca18b4c-fe8a-4b48-fb66-37a3707141da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_R.to(device)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZhGJxfN_uZ1"
      },
      "source": [
        "loss = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(model_R.parameters(), lr=1e-1, momentum=9e-1)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5OSkmNU5Svg",
        "outputId": "b1b2e83a-2802-41f1-9a64-187f75351df7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        }
      },
      "source": [
        "load_path = None\n",
        "checkpoints= True\n",
        "out_dirs = '/content/drive/My Drive/task2/model_R'\n",
        "\n",
        "\n",
        "\n",
        "start_epoch = 0\n",
        "\n",
        "if load_path is not None:\n",
        "    state_dict = torch.load(load_path)\n",
        "    model.load_state_dict(state_dict)\n",
        "else:\n",
        "    train_network(network=model_R, loss=loss, optimizer=optimizer, train_iter=train_R_iter, val_iter=val_R_iter,\n",
        "                  num_epochs=10, device=device, start_epoch=start_epoch, checkpoints=checkpoints, \n",
        "                  out_dir=out_dirs)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.250, Acc: 0.193: 100%|██████████| 400/400 [02:35<00:00,  2.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 2.154, Acc: 0.241: 100%|██████████| 80/80 [00:20<00:00,  3.96it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 1.613, Acc: 0.414: 100%|██████████| 400/400 [02:33<00:00,  2.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 1.720, Acc: 0.413: 100%|██████████| 80/80 [00:20<00:00,  3.92it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 1.156, Acc: 0.594: 100%|██████████| 400/400 [02:33<00:00,  2.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 1.131, Acc: 0.607: 100%|██████████| 80/80 [00:19<00:00,  4.05it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.822, Acc: 0.714: 100%|██████████| 400/400 [02:34<00:00,  2.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 1.150, Acc: 0.628: 100%|██████████| 80/80 [00:19<00:00,  4.01it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.635, Acc: 0.780: 100%|██████████| 400/400 [02:34<00:00,  2.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.991, Acc: 0.692: 100%|██████████| 80/80 [00:19<00:00,  4.01it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.494, Acc: 0.828: 100%|██████████| 400/400 [02:34<00:00,  2.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.759, Acc: 0.751: 100%|██████████| 80/80 [00:20<00:00,  3.97it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.378, Acc: 0.868: 100%|██████████| 400/400 [02:34<00:00,  2.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.743, Acc: 0.762: 100%|██████████| 80/80 [00:19<00:00,  4.04it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.269, Acc: 0.905: 100%|██████████| 400/400 [02:34<00:00,  2.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.804, Acc: 0.761: 100%|██████████| 80/80 [00:19<00:00,  4.09it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.182, Acc: 0.938: 100%|██████████| 400/400 [02:34<00:00,  2.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.783, Acc: 0.782: 100%|██████████| 80/80 [00:19<00:00,  4.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.112, Acc: 0.962: 100%|██████████| 400/400 [02:32<00:00,  2.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.927, Acc: 0.764: 100%|██████████| 80/80 [00:19<00:00,  4.09it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best epoch 9 , validation _accuracy 78.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKquw7ot6BpI"
      },
      "source": [
        "for batch_x, batch_y in val_R_iter:\n",
        "    batch_x = batch_x.to(device)\n",
        "    labels = batch_y.cpu().data.numpy()\n",
        "    predicted_labels = model_R(batch_x)\n",
        "    predicted_labels = torch.argmax(predicted_labels, dim=-1)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TV8U_rKcYi3"
      },
      "source": [
        "model_R_report = dict()\n",
        "\n",
        "for c in range(10):\n",
        "  model_R_report[c] = recall_(labels, predicted_labels, c)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqtXiG40cu2F",
        "outputId": "b73ac2f2-18c4-445b-ea77-ea0cc1d28a9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "model_R_report"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 1.0,\n",
              " 1: 0.8888888888888888,\n",
              " 2: 0.6,\n",
              " 3: 0.9333333333333333,\n",
              " 4: 0.8333333333333334,\n",
              " 5: 0.3,\n",
              " 6: 0.7692307692307693,\n",
              " 7: 0.9411764705882353,\n",
              " 8: 0.75,\n",
              " 9: 0.9166666666666666}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4roFUKSmAOfY"
      },
      "source": [
        "Model 3 : Using only Green Channel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSmYFkxAASuh",
        "outputId": "cb59a547-81a0-49b1-8362-e0d79a2782ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\n",
        "\n",
        "dataset = torchvision.datasets.CIFAR10\n",
        "data_dir = '/content/drive/My Drive/task2/dataG'\n",
        "\n",
        "def copy_green_channel(data_object):\n",
        "    data_object[0], data_object[2] = data_object[1], data_object[1]\n",
        "    return data_object\n",
        "\n",
        "\n",
        "transform_green = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(copy_green_channel)\n",
        "])\n",
        "\n",
        "train_G_dataset = dataset(data_dir, train=True, download=True, transform=transform_green)\n",
        "train_G_iter = torch.utils.data.DataLoader(train_G_dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=num_workers, pin_memory=cuda)\n",
        "\n",
        "val_G_dataset = dataset(data_dir, download=True, train=False, transform=transform)\n",
        "val_G_iter = torch.utils.data.DataLoader(val_G_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
        "                                       pin_memory=cuda)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEaYbaZiAT95",
        "outputId": "1166dd32-ce48-486e-abbb-5eeb289da2c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_G = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=False)\n",
        "model_G.fc = torch.nn.Linear(512, 10)\n",
        "model_G.to(device)\n",
        "loss = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(model_G.parameters(), lr=1e-1, momentum=9e-1)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB4xV3q6AUAn",
        "outputId": "de792ecf-aa0b-4c2b-c241-848a6cce9813",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        }
      },
      "source": [
        "load_path = None\n",
        "checkpoints= True\n",
        "out_dirs = '/content/drive/My Drive/task2/model_G'\n",
        "\n",
        "\n",
        "\n",
        "start_epoch = 0\n",
        "\n",
        "if load_path is not None:\n",
        "    state_dict = torch.load(load_path)\n",
        "    model.load_state_dict(state_dict)\n",
        "else:\n",
        "    train_network(network=model_G, loss=loss, optimizer=optimizer, train_iter=train_G_iter, val_iter=val_G_iter,\n",
        "                  num_epochs=10, device=device, start_epoch=start_epoch, checkpoints=checkpoints, \n",
        "                  out_dir=out_dirs)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.172, Acc: 0.248: 100%|██████████| 400/400 [02:33<00:00,  2.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 2.309, Acc: 0.237: 100%|██████████| 80/80 [00:19<00:00,  4.07it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 1.528, Acc: 0.449: 100%|██████████| 400/400 [02:33<00:00,  2.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 3.794, Acc: 0.200: 100%|██████████| 80/80 [00:19<00:00,  4.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 1.220, Acc: 0.571: 100%|██████████| 400/400 [02:32<00:00,  2.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 1.506, Acc: 0.478: 100%|██████████| 80/80 [00:19<00:00,  4.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.959, Acc: 0.663: 100%|██████████| 400/400 [02:33<00:00,  2.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 1.266, Acc: 0.579: 100%|██████████| 80/80 [00:19<00:00,  4.11it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.751, Acc: 0.740: 100%|██████████| 400/400 [02:33<00:00,  2.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 2.942, Acc: 0.328: 100%|██████████| 80/80 [00:19<00:00,  4.03it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.595, Acc: 0.793: 100%|██████████| 400/400 [02:33<00:00,  2.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.745, Acc: 0.741: 100%|██████████| 80/80 [00:19<00:00,  4.10it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.464, Acc: 0.839: 100%|██████████| 400/400 [02:33<00:00,  2.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.754, Acc: 0.756: 100%|██████████| 80/80 [00:19<00:00,  4.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.351, Acc: 0.876: 100%|██████████| 400/400 [02:33<00:00,  2.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 1.446, Acc: 0.625: 100%|██████████| 80/80 [00:18<00:00,  4.25it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.246, Acc: 0.915: 100%|██████████| 400/400 [02:33<00:00,  2.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.845, Acc: 0.752: 100%|██████████| 80/80 [00:18<00:00,  4.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.162, Acc: 0.945: 100%|██████████| 400/400 [02:32<00:00,  2.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 1.097, Acc: 0.715: 100%|██████████| 80/80 [00:19<00:00,  4.07it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best epoch 7 , validation _accuracy 75.62\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIPw7Ad-c1Qg"
      },
      "source": [
        "for batch_x, batch_y in val_G_iter:\n",
        "    batch_x = batch_x.to(device)\n",
        "    labels = batch_y.cpu().data.numpy()\n",
        "    predicted_labels = model_G(batch_x)\n",
        "    predicted_labels = torch.argmax(predicted_labels, dim=-1)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEyUe3xKc4VY"
      },
      "source": [
        "model_G_report = dict()\n",
        "\n",
        "for c in range(10):\n",
        "  model_G_report[c] = recall_(labels, predicted_labels, c)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02s0cuFoAvQD",
        "outputId": "350cb55a-bffe-4c07-9c80-0a24f4e4de60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "model_G_report"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.7272727272727273,\n",
              " 1: 0.6363636363636364,\n",
              " 2: 0.7272727272727273,\n",
              " 3: 0.5384615384615384,\n",
              " 4: 0.5,\n",
              " 5: 1.0,\n",
              " 6: 0.5625,\n",
              " 7: 0.8181818181818182,\n",
              " 8: 0.6875,\n",
              " 9: 1.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5Eip3a-AUPT"
      },
      "source": [
        "Model 4: Using only Blue Channel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQl7M7bEdQ63",
        "outputId": "951e3e7a-242a-4b99-c878-8af340ef2ded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "dataset = torchvision.datasets.CIFAR10\n",
        "data_dir = '/content/drive/My Drive/task2/dataB'\n",
        "\n",
        "def copy_blue_channel(data_object):\n",
        "    data_object[0], data_object[1] = data_object[2], data_object[2]\n",
        "    return data_object\n",
        "\n",
        "\n",
        "transform_blue = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(copy_blue_channel)\n",
        "])\n",
        "\n",
        "train_B_dataset = dataset(data_dir, train=True, download=True, transform=transform_blue)\n",
        "train_B_iter = torch.utils.data.DataLoader(train_B_dataset, batch_size=batch_size,\n",
        "                                           shuffle=True, num_workers=num_workers, pin_memory=cuda)\n",
        "\n",
        "val_B_dataset = dataset(data_dir, download=True, train=False, transform=transform)\n",
        "val_B_iter = torch.utils.data.DataLoader(val_B_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
        "                                         pin_memory=cuda)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0KRkHh2dQ99",
        "outputId": "678121a9-4610-4642-f5ce-1ca28f311953",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_B = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=False)\n",
        "model_B.fc = torch.nn.Linear(512, 10)\n",
        "model_B.to(device)\n",
        "loss = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(model_B.parameters(), lr=1e-1, momentum=9e-1)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFpzTqTidRBX",
        "outputId": "163daab2-9476-4146-86ed-cfd1d3de369b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        }
      },
      "source": [
        "load_path = None\n",
        "checkpoints= True\n",
        "out_dirs = '/content/drive/My Drive/task2/model_B'\n",
        "\n",
        "start_epoch = 0\n",
        "\n",
        "if load_path is not None:\n",
        "    state_dict = torch.load(load_path)\n",
        "    model.load_state_dict(state_dict)\n",
        "else:\n",
        "    train_network(network=model_B, loss=loss, optimizer=optimizer, train_iter=train_B_iter, val_iter=val_B_iter,\n",
        "                  num_epochs=10, device=device, start_epoch=start_epoch, checkpoints=checkpoints, \n",
        "                  out_dir=out_dirs)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 1.997, Acc: 0.299: 100%|██████████| 400/400 [02:34<00:00,  2.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 1.710, Acc: 0.371: 100%|██████████| 80/80 [00:19<00:00,  4.06it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 1.324, Acc: 0.525: 100%|██████████| 400/400 [02:33<00:00,  2.61it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 1.360, Acc: 0.524: 100%|██████████| 80/80 [00:19<00:00,  4.04it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.918, Acc: 0.679: 100%|██████████| 400/400 [02:33<00:00,  2.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 1.029, Acc: 0.637: 100%|██████████| 80/80 [00:20<00:00,  3.99it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.693, Acc: 0.757: 100%|██████████| 400/400 [02:33<00:00,  2.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.751, Acc: 0.739: 100%|██████████| 80/80 [00:19<00:00,  4.04it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.538, Acc: 0.813: 100%|██████████| 400/400 [02:35<00:00,  2.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.698, Acc: 0.759: 100%|██████████| 80/80 [00:19<00:00,  4.03it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.414, Acc: 0.856: 100%|██████████| 400/400 [02:34<00:00,  2.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.738, Acc: 0.767: 100%|██████████| 80/80 [00:20<00:00,  3.89it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.296, Acc: 0.897: 100%|██████████| 400/400 [02:34<00:00,  2.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.844, Acc: 0.766: 100%|██████████| 80/80 [00:20<00:00,  3.91it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.209, Acc: 0.927: 100%|██████████| 400/400 [02:35<00:00,  2.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.883, Acc: 0.761: 100%|██████████| 80/80 [00:20<00:00,  3.98it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.136, Acc: 0.953: 100%|██████████| 400/400 [02:34<00:00,  2.59it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.954, Acc: 0.750: 100%|██████████| 80/80 [00:20<00:00,  3.97it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10 \n",
            "Training \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.090, Acc: 0.970: 100%|██████████| 400/400 [02:35<00:00,  2.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loss: 0.969, Acc: 0.774: 100%|██████████| 80/80 [00:19<00:00,  4.01it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best epoch 10 , validation _accuracy 77.42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeDUoOxhdREw"
      },
      "source": [
        ""
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCxCASMXAXww"
      },
      "source": [
        "for batch_x, batch_y in val_B_iter:\n",
        "    batch_x = batch_x.to(device)\n",
        "    labels = batch_y.cpu().data.numpy()\n",
        "    predicted_labels = model_B(batch_x)\n",
        "    predicted_labels = torch.argmax(predicted_labels, dim=-1)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PMWWB2-dBvE"
      },
      "source": [
        "model_B_report = dict()\n",
        "\n",
        "for c in range(10):\n",
        "  model_B_report[c] = recall_(labels, predicted_labels, c)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9Dmww4HnNmm",
        "outputId": "0cc1ed3c-41bc-4630-96d0-6d9970d9626b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "model_B_report"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.75,\n",
              " 1: 0.8333333333333334,\n",
              " 2: 0.6153846153846154,\n",
              " 3: 1.0,\n",
              " 4: 0.7692307692307693,\n",
              " 5: 0.3,\n",
              " 6: 0.8571428571428571,\n",
              " 7: 1.0,\n",
              " 8: 0.9,\n",
              " 9: 0.9}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz6O9JFynPVu"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQgLFK2MBFBL",
        "outputId": "0d6b86e6-0ed8-47ba-8e14-27f891f4248b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "\n",
        "columns = ['Model ', 'Model R', 'Model G','Model B' ]\n",
        "cell_text = []\n",
        "cell_text.append(list(model_1_report.values()))\n",
        "cell_text.append(list(model_R_report.values()))\n",
        "cell_text.append(list(model_G_report.values()))\n",
        "cell_text.append(list(model_B_report.values()))\n",
        "cell_text = np.transpose(cell_text)\n",
        "pd.DataFrame(cell_text, columns=columns)\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Model R</th>\n",
              "      <th>Model G</th>\n",
              "      <th>Model B</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.833333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.615385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.941176</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.769231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.352941</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.857143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.941176</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.944444</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Model    Model R   Model G   Model B\n",
              "0  0.833333  1.000000  0.727273  0.750000\n",
              "1  0.888889  0.888889  0.636364  0.833333\n",
              "2  0.800000  0.600000  0.727273  0.615385\n",
              "3  0.941176  0.933333  0.538462  1.000000\n",
              "4  0.666667  0.833333  0.500000  0.769231\n",
              "5  0.352941  0.300000  1.000000  0.300000\n",
              "6  0.750000  0.769231  0.562500  0.857143\n",
              "7  0.800000  0.941176  0.818182  1.000000\n",
              "8  0.800000  0.750000  0.687500  0.900000\n",
              "9  0.944444  0.916667  1.000000  0.900000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g4plKhdDjhY"
      },
      "source": [
        ""
      ],
      "execution_count": 41,
      "outputs": []
    }
  ]
}